\documentclass{scrartcl}

% --- Packages and Modern Fonts ---
\usepackage{fontspec}
\usepackage[margin=0.75in]{geometry}

\usepackage[dvipsnames,svgnames]{xcolor}
% Define custom colors (replace the hex values with your brand colors)
\definecolor{LlamaPrimary}{HTML}{003366} % Example: dark blue
\definecolor{LlamaAccent}{HTML}{CC0000}  % Example: red

% --- Header and Footer ---
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Llama 4 Benchmark Report}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

% --- Hyperlink Styling ---
\usepackage[colorlinks, urlcolor=LlamaPrimary, linkcolor=LlamaAccent, citecolor=LlamaPrimary]{hyperref}

% --- Sectioning ---
\usepackage{titlesec}
\titleformat{\section}
  {\normalfont\Large\bfseries\color{LlamaPrimary}}
  {\thesection}{1em}{} % Keep sections unnumbered as per original
\titlespacing*{\section}{0pt}{3.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{LlamaPrimary}}
  {\thesubsection}{1em}{} % Keep subsections unnumbered
\titlespacing*{\subsection}{0pt}{3ex plus 1ex minus .2ex}{1.5ex plus .2ex}
% Make section* look like section
\titleformat{name=\section,numberless}
  {\normalfont\Large\bfseries\color{LlamaPrimary}}
  {}{0em}{}
\titlespacing*{name=\section,numberless}{0pt}{3.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}
% Make subsection* look like subsection
\titleformat{name=\subsection,numberless}
  {\normalfont\large\bfseries\color{LlamaPrimary}}
  {}{0em}{}
\titlespacing*{name=\subsection,numberless}{0pt}{3ex plus 1ex minus .2ex}{1.5ex plus .2ex}

% --- List Customization ---
\usepackage{enumitem}
\setlist[itemize,1]{label=\textcolor{LlamaAccent}{\textbullet}, leftmargin=*}
\setlist[description]{font=\normalfont\bfseries, style=unboxed, leftmargin=2em, itemsep=0.5ex}

% --- Table Packages ---
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{siunitx}
\usepackage{threeparttable}
\usepackage{caption} % Better caption control
\usepackage{float}   % Use [H] for precise float placement

% --- siunitx Setup ---
\sisetup{
    detect-weight, % Detect \bfseries
    mode=text,     % Text mode formatting
    group-separator={}, % No thousands separator
    table-align-text-post=false % Align based on numbers
}
% Define column types
\newcolumntype{L}{>{\raggedright\arraybackslash}X} % Wrapped text column
% Define S columns with custom format via T
\newcolumntype{T}[1]{S[table-format=#1, detect-weight, mode=text]}

\begin{document}

% Set default font if using fontspec
% \setDefaultFonts % Uncomment if you defined fonts above

\section*{Llama 4 Benchmark and Model Comparison Report}

\bigskip

\section*{Llama 4: Leading intelligence. Unrivaled speed and efficiency.}

The most accessible and scalable generation of Llama is here. Native multimodality, mixture-of-experts models, super long context windows, step changes in performance, and unparalleled efficiency---all in easy-to-deploy sizes custom fit for how you want to use it.

\bigskip

\section*{Model Cards}

\begin{description}[font=\normalfont\bfseries\color{LlamaPrimary}]
    \item[Llama 4 Scout] A class-leading natively multimodal model that offers superior text and visual intelligence, efficient single H100 GPU performance, and a 10M context window for seamless long document analysis.
    \item[Llama 4 Maverick] An industry-leading multimodal model for image and text understanding that delivers groundbreaking intelligence and fast responses at a low cost.
    \item[Llama 4 Behemoth Preview] An early preview (itâ€™s still training!) of the Llama 4 teacher model used to distill Llama 4 Scout and Llama 4 Maverick.
\end{description}

\bigskip

\section*{Key Features}

\begin{itemize}
    \item \textbf{Natively Multimodal} --- Llama 4 models leverage early fusion by pre-training on large amounts of unlabeled text and vision tokens, marking a significant step forward from separate, frozen multimodal weights.
    \item \textbf{Advanced Problem Solving} --- Both Llama 4 Scout and Llama 4 Maverick tackle intricate problems, offering intelligent solutions across complex domains.
    \item \textbf{Unparalleled Long Context} --- With Llama 4 Scout supporting up to 10M tokens of context (the longest available in the industry), new use cases in memory, personalization, and multimodal applications become possible.
    \item \textbf{Expert Image Grounding} --- These models excel in aligning user prompts with relevant visual concepts, anchoring responses to specific image regions.
    \item \textbf{Multilingual Writing} --- Pre-trained and fine-tuned for robust text understanding across 12 languages, Llama 4 supports global development and deployment.
\end{itemize}

\bigskip

\section*{Individual Model Benchmark Tables}

% --- Gemini 2.5 Pro Experimental 03-25 Table ---
\subsection*{Gemini 2.5 Pro Experimental 03-25 (Source: \url{https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/\#enhanced-reasoning})}
\begin{table}[H]
    \setlength{\tabcolsep}{4pt}
    \begin{threeparttable}
        \caption{Benchmark Comparison Featuring Gemini 2.5 Pro Exp. 03-25}
        \label{tab:gemini-comp}
        \small
        \begin{tabularx}{\textwidth}{@{} L T{2.1} T{2.1} T{2.1} T{2.1} T{2.1} T{2.1} @{}}
            \toprule
            \textbf{Benchmark}                & {\textbf{Gemini 2.5 Pro}}       & {\textbf{o3-mini}}  & {\textbf{GPT-4.5}} & {\textbf{Claude 3.7}} & {\textbf{Grok 3 Beta}} & {\textbf{DeepSeek R1}} \\
            \midrule
            \multicolumn{7}{@{}l}{\textit{Reasoning \& knowledge}}                                                                                                                                   \\[1ex]
            Humanity's Last Exam (no tools)   & 18.8                            & 14.0\tnote{*}       & 6.4                & 8.9                   & {--}                   & 8.6\tnote{*}           \\
            \addlinespace
            \multicolumn{7}{@{}l}{\textit{Science}}                                                                                                                                                  \\[1ex]
            GPQA diamond (single, pass@1)     & 84.0                            & 79.7                & 71.4               & 78.2                  & 80.2                   & 71.5                   \\
            GPQA diamond (multiple)           & {--}                            & {--}                & {--}               & \bfseries 84.8        & \bfseries 84.6         & {--}                   \\
            \addlinespace
            \multicolumn{7}{@{}l}{\textit{Mathematics}}                                                                                                                                              \\[1ex]
            AIME 2025 (single, pass@1)        & \bfseries 86.7                  & 86.5                & {--}               & 49.5                  & 77.3                   & 70.0                   \\
            AIME 2025 (multiple)              & {--}                            & {--}                & {--}               & {--}                  & \bfseries 93.3         & {--}                   \\
            AIME 2024 (single, pass@1)        & \bfseries 92.0                  & 87.3                & 36.7               & 61.3                  & 83.9                   & 79.8                   \\
            AIME 2024 (multiple)              & {--}                            & {--}                & {--}               & \bfseries 80.0        & \bfseries 93.3         & {--}                   \\
            \addlinespace
            \multicolumn{7}{@{}l}{\textit{Code generation}}                                                                                                                                          \\[1ex]
            LiveCodeBench v5 (single, pass@1) & 70.4                            & \bfseries 74.1      & {--}               & {--}                  & 70.6                   & 64.3                   \\
            LiveCodeBench v5 (multiple)       & {--}                            & {--}                & {--}               & {--}                  & \bfseries 79.4         & {--}                   \\
            \addlinespace
            \multicolumn{7}{@{}l}{\textit{Code editing}}                                                                                                                                             \\[1ex]
            Aider Polyglot                    & \multicolumn{1}{c}{74.0 / 68.6} & 60.4\tnote{d}       & 44.9\tnote{d}      & 64.9\tnote{d}         & {--}                   & 56.9\tnote{d}          \\
            \addlinespace
            \multicolumn{7}{@{}l}{\textit{Agentic coding}}                                                                                                                                           \\[1ex]
            SWE-bench verified                & 63.8                            & 49.3                & 38.0               & \bfseries 70.3        & {--}                   & 49.2                   \\
            \addlinespace
            \multicolumn{7}{@{}l}{\textit{Factuality}}                                                                                                                                               \\[1ex]
            SimpleQA                          & \bfseries 52.9                  & 13.8                & 62.5               & {--}                  & 43.6                   & 30.1                   \\
            \addlinespace
            \multicolumn{7}{@{}l}{\textit{Visual reasoning}}                                                                                                                                         \\[1ex]
            MMMU (single)                     & \bfseries 81.7                  & {No MM}\tnote{\dag} & 74.4               & 75.0                  & 76.0                   & {No MM}\tnote{\dag}    \\
            MMMU (multiple)                   & {--}                            & {--}                & {--}               & {--}                  & \bfseries 78.0         & {--}                   \\
            \addlinespace
            \multicolumn{7}{@{}l}{\textit{Image understanding}}                                                                                                                                      \\[1ex]
            Vibe-Eval (Reka)                  & 69.4                            & {No MM}\tnote{\dag} & {--}               & {--}                  & {--}                   & {No MM}\tnote{\dag}    \\
            \addlinespace
            \multicolumn{7}{@{}l}{\textit{Long context}}                                                                                                                                             \\[1ex]
            MRCR (128k avg)                   & \bfseries 94.5                  & 61.4                & 64.0               & {--}                  & {--}                   & {--}                   \\
            MRCR (1M pointwise)               & \bfseries 83.1                  & {--}                & {--}               & {--}                  & {--}                   & {--}                   \\
            \addlinespace
            \multicolumn{7}{@{}l}{\textit{Multilingual performance}}                                                                                                                                 \\[1ex]
            Global MMLU (Lite)                & \bfseries 89.8                  & {--}                & {--}               & {--}                  & {--}                   & {--}                   \\
            \bottomrule
        \end{tabularx}
        \begin{tablenotes}[para,flushleft]
            \footnotesize
            \item[*] Text problems only.
            \item[d] Diff performance.
            \item[\dag] No multimodal support reported/applicable.
            \item pass@1: Single attempt. Multiple: Multiple attempts/voting. Gemini results: default sampling (pass@1), model \texttt{gemini-2.5-pro-exp-03-25}. Non-Gemini results: self-reported. Sources include \url{https://agi.safe.ai/}, \url{https://matharena.ai/}, \url{https://livecodebench.github.io/}, \url{https://aider.chat/docs/leaderboards}. Source for Gemini table: \url{https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/\#enhanced-reasoning}.
        \end{tablenotes}
    \end{threeparttable}
\end{table}

% --- Llama 4 Maverick Table ---
\subsection*{Llama 4 Maverick (Source: \url{https://www.llama.com})}
\begin{table}[H]
    \setlength{\tabcolsep}{4pt}
    \begin{threeparttable}
        \caption{Llama 4 Maverick Benchmark Comparison}
        \label{tab:llama4-maverick}
        \small
        \begin{tabularx}{\textwidth}{@{} L T{2.1} T{2.1} >{\centering\arraybackslash}X T{2.1} @{}}
            \toprule
            \textbf{Category / Benchmark} & {\textbf{Llama 4 Maverick}}                                    & {\textbf{Gemini 2.0 Flash}}      & {\textbf{DeepSeek v3.1}}                           & {\textbf{GPT-4o}}                \\
            \midrule
            \multicolumn{5}{@{}l}{\textit{Inference Cost (\$/1M tokens In/Out)}}                                                                                                                                                      \\[1ex]
                                          & \multicolumn{1}{c}{\bfseries\$\num{0.19}--\num{0.49}\tnote{a}} & \multicolumn{1}{c}{\$\num{0.17}} & \multicolumn{1}{c}{\$\num{0.48}}                   & \multicolumn{1}{c}{\$\num{4.38}} \\
            \addlinespace
            \multicolumn{5}{@{}l}{\textit{Image Reasoning}}                                                                                                                                                                           \\[1ex]
            MMMU                          & 73.4                                                           & 71.7                             & {No MM}\tnote{\dag}                                & 69.1                             \\
            MathVista                     & 73.7                                                           & 73.1                             & {No MM}\tnote{\dag}                                & 63.8                             \\
            \addlinespace
            \multicolumn{5}{@{}l}{\textit{Image Understanding}}                                                                                                                                                                       \\[1ex]
            ChartQA                       & \bfseries 90.0                                                 & 88.3                             & {--}                                               & 85.7                             \\
            DocVQA (test)                 & \bfseries 94.4                                                 & {--}                             & {--}                                               & 92.8                             \\
            \addlinespace
            \multicolumn{5}{@{}l}{\textit{Coding}}                                                                                                                                                                                    \\[1ex]
            LiveCodeBench (10/24--02/25)  & \bfseries 43.4                                                 & 34.5                             & \multicolumn{1}{c}{\bfseries 45.8 / 49.2\tnote{b}} & 32.3                             \\
            \addlinespace
            \multicolumn{5}{@{}l}{\textit{Reasoning \& Knowledge}}                                                                                                                                                                    \\[1ex]
            MMLU Pro                      & \bfseries 80.5                                                 & 77.6                             & \bfseries 81.2                                     & {--}                             \\
            GPQA Diamond                  & \bfseries 69.8                                                 & 60.1                             & 68.4                                               & 53.6                             \\
            \addlinespace
            \multicolumn{5}{@{}l}{\textit{Multilingual}}                                                                                                                                                                              \\[1ex]
            Multilingual MMLU             & \bfseries 84.6                                                 & {--}                             & {--}                                               & 81.5                             \\
            \addlinespace
            \multicolumn{5}{@{}l}{\textit{Long Context}}                                                                                                                                                                              \\[1ex]
            MTOB (half book)              & \multicolumn{1}{c}{\bfseries 54.0 / 46.4}                      & \multicolumn{1}{c}{48.4 / 39.8}  & {128k context}\tnote{\ddag}                        & {128k context}\tnote{\ddag}      \\
            MTOB (full book)              & \multicolumn{1}{c}{\bfseries 50.8 / 46.7}                      & \multicolumn{1}{c}{45.5 / 39.6}  & {128k context}\tnote{\ddag}                        & {128k context}\tnote{\ddag}      \\
            \bottomrule
        \end{tabularx}
        \begin{tablenotes}[para,flushleft]
            \footnotesize
            \item[a] \$0.19/1Mtok (3:1 blended) estimated distributed inference cost.
            \item[b] DeepSeek v3.1 internal result (45.8) used as range unknown.
            \item[\dag] No multimodal support reported/applicable.
            \item[\ddag] Context window limits reported result.
            \item Llama results: 0-shot, temp=0, averaged for high-variance. Non-Llama: highest self-reported. Cost estimates (non-Llama): Artificial Analysis. Source: \url{https://www.llama.com}
        \end{tablenotes}
    \end{threeparttable}
\end{table}

% --- Llama 4 Scout Table ---
\subsection*{Llama 4 Scout (Source: \url{https://www.llama.com})}
\begin{table}[H]
    \setlength{\tabcolsep}{4pt}
    \begin{threeparttable}
        \caption{Llama 4 Scout Benchmark Comparison}
        \label{tab:llama4-scout}
        \small
        \begin{tabularx}{\textwidth}{@{} L T{2.1} >{\centering\arraybackslash}X >{\centering\arraybackslash}X T{2.1} T{2.1} T{2.1} @{}}
            \toprule
            \textbf{Category / Benchmark} & {\textbf{Llama 4 Scout}}                  & {\textbf{Llama 3.3 70B}}    & {\textbf{Llama 3.1 405B}}   & {\textbf{Gemma 3 (27B)}}    & {\textbf{Mistral 3.1 (24B)}} & {\textbf{Gemini 2.0 Flash-Lite}} \\
            \midrule
            \multicolumn{7}{@{}l}{\textit{Image Reasoning}}                                                                                                                                                                                       \\[1ex]
            MMMU                          & 69.4                                      & {--}                        & {--}                        & 64.9                        & 62.8                         & 68.6                             \\
            MathVista                     & 70.7                                      & {--}                        & {--}                        & 67.6                        & 68.9                         & 57.6                             \\
            \addlinespace
            \multicolumn{7}{@{}l}{\textit{Image Understanding}}                                                                                                                                                                                   \\[1ex]
            ChartQA                       & 88.8                                      & {No MM}\tnote{\dag}         & {No MM}\tnote{\dag}         & 76.3                        & \bfseries 86.2               & 73.0                             \\
            DocVQA                        & \bfseries 94.4                            & {--}                        & {--}                        & 90.4                        & \bfseries 94.1               & 91.2                             \\
            \addlinespace
            \multicolumn{7}{@{}l}{\textit{Coding}}                                                                                                                                                                                                \\[1ex]
            LiveCodeBench (10/24--02/25)  & 32.8                                      & \bfseries 33.3              & 27.7                        & 29.7                        & {--}                         & 28.9                             \\
            \addlinespace
            \multicolumn{7}{@{}l}{\textit{Reasoning \& Knowledge}}                                                                                                                                                                                \\[1ex]
            MMLU Pro                      & 74.3                                      & 68.9                        & 73.4                        & 67.5                        & 66.8                         & 71.6                             \\
            GPQA Diamond                  & \bfseries 57.2                            & 50.5                        & 49.0                        & 42.4                        & 46.0                         & 51.5                             \\
            \addlinespace
            \multicolumn{7}{@{}l}{\textit{Long Context}}                                                                                                                                                                                          \\[1ex]
            MTOB (half book)              & \multicolumn{1}{c}{\bfseries 42.2 / 36.6} & {128k context}\tnote{\ddag} & {128k context}\tnote{\ddag} & {128k context}\tnote{\ddag} & {128k context}\tnote{\ddag}  & \multicolumn{1}{c}{42.3 / 35.1}  \\
            MTOB (full book)              & \multicolumn{1}{c}{\bfseries 39.7 / 36.3} & {--}                        & {--}                        & {--}                        & {--}                         & \multicolumn{1}{c}{35.1 / 30.0}  \\
            \bottomrule
        \end{tabularx}
        \begin{tablenotes}[para,flushleft]
            \footnotesize
            \item[\dag] No multimodal support reported/applicable.
            \item[\ddag] Context window limits reported result.
            \item Llama results: 0-shot, temp=0, averaged for high-variance. Non-Llama: highest self-reported. Source: \url{https://www.llama.com}
        \end{tablenotes}
    \end{threeparttable}
\end{table}

% --- Llama 4 Behemoth Table ---
\subsection*{Llama 4 Behemoth (Source: \url{https://www.llama.com})}
\begin{table}[H]
    \setlength{\tabcolsep}{4pt}
    \begin{threeparttable}
        \caption{Llama 4 Behemoth Benchmark Comparison (Preview)}
        \label{tab:llama4-behemoth}
        \small
        \begin{tabularx}{\textwidth}{@{} L T{2.1} T{2.1} T{2.1} T{2.1} @{}}
            \toprule
            \textbf{Category / Benchmark} & {\textbf{Llama 4 Behemoth}} & {\textbf{Claude Sonnet 3.7}} & {\textbf{Gemini 2.0 Pro}} & {\textbf{GPT-4.5}} \\
            \midrule
            \multicolumn{5}{@{}l}{\textit{Coding}}                                                                                                      \\[1ex]
            LiveCodeBench (10/24--02/25)  & \bfseries 49.4              & {--}                         & 36.0                      & {--}               \\
            \addlinespace
            \multicolumn{5}{@{}l}{\textit{Reasoning \& Knowledge}}                                                                                      \\[1ex]
            MATH-500                      & \bfseries 95.0              & 82.2                         & 91.8                      & {--}               \\
            MMLU Pro                      & \bfseries 82.2              & {--}                         & 79.1                      & {--}               \\
            GPQA Diamond                  & \bfseries 73.7              & 68.0                         & 64.7                      & 71.4               \\
            \addlinespace
            \multicolumn{5}{@{}l}{\textit{Multilingual}}                                                                                                \\[1ex]
            Multilingual MMLU (OpenAI)    & \bfseries 85.8              & 83.2                         & {--}                      & 85.1               \\
            \addlinespace
            \multicolumn{5}{@{}l}{\textit{Image Reasoning}}                                                                                             \\[1ex]
            MMMU                          & \bfseries 76.1              & 71.8                         & 72.7                      & 74.4               \\
            \bottomrule
        \end{tabularx}
        \begin{tablenotes}[para,flushleft]
            \footnotesize
            \item Llama results: Current best internal runs (preview model). Non-Llama: highest self-reported. Source: \url{https://www.llama.com}
        \end{tablenotes}
    \end{threeparttable}
\end{table}

% --- Combined Model Benchmark Comparison ---
\subsection*{Combined Model Benchmark Comparison}
\begin{table}[H]
    \setlength{\tabcolsep}{4pt}
    \begin{threeparttable}
        \caption{Combined Model Benchmark Comparison (GPQA Diamond \& MMLU)}
        \label{tab:combined-models}
        \small
        \begin{tabularx}{\textwidth}{@{} L T{2.1} T{2.1} @{}}
            \toprule
            \textbf{Model}          & \textbf{GPQA Diamond} & \textbf{MMLU / Global MMLU} \\
            \midrule
            Gemini 2.5 Pro Exp03-25 & 84.0                  & 89.8                        \\
            o3-mini                 & 79.7                  & \multicolumn{1}{c}{--}      \\
            GPT-4.5                 & 71.4                  & \multicolumn{1}{c}{--}      \\
            Claude 3.7              & 78.2                  & \multicolumn{1}{c}{--}      \\
            Grok 3 Beta             & 80.2                  & \multicolumn{1}{c}{--}      \\
            DeepSeek R1             & 71.5                  & \multicolumn{1}{c}{--}      \\
            Llama 4 Maverick        & 69.8                  & 80.5                        \\
            Gemini 2.0 Flash        & 60.1                  & \multicolumn{1}{c}{--}      \\
            DeepSeek v3.1           & 68.4                  & \multicolumn{1}{c}{--}      \\
            GPT-4o                  & 53.6                  & \multicolumn{1}{c}{--}      \\
            Llama 4 Scout           & 57.2                  & 74.3                        \\
            Llama 3.3 70B           & 50.5                  & \multicolumn{1}{c}{--}      \\
            Llama 3.1 405B          & 49.0                  & \multicolumn{1}{c}{--}      \\
            Gemma 3 (27B)           & 42.4                  & \multicolumn{1}{c}{--}      \\
            Mistral 3.1 (24B)       & 46.0                  & \multicolumn{1}{c}{--}      \\
            Gemini 2.0 Flash-Lite   & 51.5                  & \multicolumn{1}{c}{--}      \\
            Llama 4 Behemoth        & 73.7                  & 82.2                        \\
            Claude Sonnet 3.7       & 68.0                  & \multicolumn{1}{c}{--}      \\
            Gemini 2.0 Pro          & 64.7                  & \multicolumn{1}{c}{--}      \\
            \bottomrule
        \end{tabularx}
        \begin{tablenotes}[para,flushleft]
            \footnotesize
            \item GPQA Diamond: Single attempt (pass@1).
            \item MMLU / Global MMLU: MMLU Pro or Global MMLU (Lite) where available.
        \end{tablenotes}
    \end{threeparttable}
\end{table}

\end{document}
